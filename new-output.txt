================================================================================
File: /Users/jluker/mi-experiments/emphasis_experiment.log
================================================================================
2024-11-17 00:39:21 - __main__ - INFO - Loading configuration from configs/example_emphasis_config.json
2024-11-17 00:39:21 - __main__ - INFO - Loading model...
2024-11-17 00:40:04 - __main__ - INFO - Loading configuration from configs/example_emphasis_config.json
2024-11-17 00:40:04 - __main__ - INFO - Loading model...
2024-11-17 00:40:33 - __main__ - INFO - Loading configuration from configs/example_emphasis_config.json
2024-11-17 00:40:33 - __main__ - INFO - Loading model...
2024-11-17 00:41:33 - __main__ - INFO - Loading configuration from configs/example_emphasis_config.json
2024-11-17 00:41:33 - __main__ - INFO - Loading model...
2024-11-17 00:41:38 - __main__ - INFO - Loading configuration from configs/example_emphasis_config.json
2024-11-17 00:41:38 - __main__ - INFO - Loading model...


================================================================================
File: /Users/jluker/mi-experiments/new-output.txt
================================================================================


================================================================================
File: /Users/jluker/mi-experiments/LICENSE
================================================================================
MIT License

Copyright (c) 2024 [Joseph Luker](https://github.com/JoeLuker)

This software builds upon:

- MLX ParaLLM (Copyright (c) 2024 willccbb)
- MLX Examples (Copyright (c) 2023 Apple Inc.)
- repeng (Copyright (c) 2024 Theia Vogel)
- representation-engineering (Copyright (c) 2024 Andy Zou)

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================================================
File: /Users/jluker/mi-experiments/requirements.txt
================================================================================
mlx_lm
mlx
transformers
--extra-index-url https://download.pytorch.org/whl/cpu
torch
pytest


================================================================================
File: /Users/jluker/mi-experiments/diff.txt
================================================================================
diff --git a/Users/jluker/mi-experiments/.venv/lib/python3.13/site-packages/mlx/nn/layers/base.py b/Users/jluker/mlx_parallm/.venv/lib/python3.12/site-packages/mlx/nn/layers/base.py
index e493c85..ffb614a 100644
--- a/Users/jluker/mi-experiments/.venv/lib/python3.13/site-packages/mlx/nn/layers/base.py
+++ b/Users/jluker/mlx_parallm/.venv/lib/python3.12/site-packages/mlx/nn/layers/base.py
@@ -1,7 +1,5 @@
 # Copyright Â© 2023 Apple Inc.
 
-from __future__ import annotations
-
 import textwrap
 from typing import Any, Callable, List, Optional, Tuple, Union
 
@@ -9,6 +7,42 @@ import mlx.core as mx
 from mlx.utils import tree_flatten, tree_unflatten
 
 
+def _unwrap(model, value_key, value, filter_fn, map_fn, is_leaf_fn):
+    if is_leaf_fn(model, value_key, value):
+        return map_fn(value)
+
+    elif isinstance(value, Module):
+        return {
+            k: _unwrap(value, k, v, filter_fn, map_fn, is_leaf_fn)
+            for k, v in value.items()
+            if filter_fn(value, k, v)
+        }
+
+    elif isinstance(value, dict):
+        nd = {}
+        for k, v in value.items():
+            tk = f"{value_key}.{k}"
+            nd[k] = (
+                _unwrap(model, tk, v, filter_fn, map_fn, is_leaf_fn)
+                if filter_fn(model, tk, v)
+                else {}
+            )
+        return nd
+
+    elif isinstance(value, list):
+        nl = []
+        for i, vi in enumerate(value):
+            tk = f"{value_key}.{i}"
+            nl.append(
+                _unwrap(model, tk, vi, filter_fn, map_fn, is_leaf_fn)
+                if filter_fn(model, tk, vi)
+                else {}
+            )
+        return nl
+
+    raise RuntimeError("Unexpected leaf found while traversing the module")
+
+
 class Module(dict):
     """Base class for building neural networks with MLX.
 
@@ -112,13 +146,12 @@ class Module(dict):
             self[key] = val
         else:
             super(Module, self).__setattr__(key, val)
-            self.pop(key, None)
 
     def load_weights(
         self,
         file_or_weights: Union[str, List[Tuple[str, mx.array]]],
         strict: bool = True,
-    ) -> Module:
+    ) -> "Module":
         """
         Update the model's weights from a ``.npz``, a ``.safetensors`` file, or a list.
 
@@ -173,6 +206,7 @@ class Module(dict):
         if strict:
             new_weights = dict(weights)
             curr_weights = dict(tree_flatten(self.parameters()))
+            
             if extras := (new_weights.keys() - curr_weights.keys()):
                 extras = " ".join(extras)
                 raise ValueError(f"Received parameters not in model: {extras}.")
@@ -233,9 +267,9 @@ class Module(dict):
 
     def filter_and_map(
         self,
-        filter_fn: Callable[[Module, str, Any], bool],
+        filter_fn: Callable[["mlx.nn.Module", str, Any], bool],
         map_fn: Optional[Callable] = None,
-        is_leaf_fn: Optional[Callable[[Module, str, Any], bool]] = None,
+        is_leaf_fn: Optional[Callable[["mlx.nn.Module", str, Any], bool]] = None,
     ):
         """Recursively filter the contents of the module using ``filter_fn``,
         namely only select keys and values where ``filter_fn`` returns true.
@@ -290,7 +324,7 @@ class Module(dict):
 
         return self.filter_and_map(self.valid_child_filter, is_leaf_fn=_is_leaf_module)
 
-    def update(self, parameters: dict) -> Module:
+    def update(self, parameters: dict) -> "Module":
         """Replace the parameters of this Module with the provided ones in the
         dict of dicts and lists.
 
@@ -338,8 +372,8 @@ class Module(dict):
     def apply(
         self,
         map_fn: Callable[[mx.array], mx.array],
-        filter_fn: Optional[Callable[[Module, str, Any], bool]] = None,
-    ) -> Module:
+        filter_fn: Optional[Callable[["mlx.nn.Module", str, Any], bool]] = None,
+    ) -> "Module":
         """Map all the parameters using the provided ``map_fn`` and immediately
         update the module with the mapped parameters.
 
@@ -358,7 +392,7 @@ class Module(dict):
         self.update(self.filter_and_map(filter_fn, map_fn))
         return self
 
-    def update_modules(self, modules: dict) -> Module:
+    def update_modules(self, modules: dict) -> "Module":
         """Replace the child modules of this :class:`Module` instance with the
         provided ones in the dict of dicts and lists.
 
@@ -399,7 +433,9 @@ class Module(dict):
         apply(self, modules)
         return self
 
-    def apply_to_modules(self, apply_fn: Callable[[str, Module], Any]) -> Module:
+    def apply_to_modules(
+        self, apply_fn: Callable[[str, "mlx.nn.Module"], Any]
+    ) -> "Module":
         """Apply a function to all the modules in this instance (including this
         instance).
 
@@ -454,7 +490,7 @@ class Module(dict):
         recurse: bool = True,
         keys: Optional[Union[str, List[str]]] = None,
         strict: bool = False,
-    ) -> Module:
+    ) -> "Module":
         """Freeze the Module's parameters or some of them. Freezing a parameter means not
         computing gradients for it.
 
@@ -509,7 +545,7 @@ class Module(dict):
         recurse: bool = True,
         keys: Optional[Union[str, List[str]]] = None,
         strict: bool = False,
-    ) -> Module:
+    ) -> "Module":
         """Unfreeze the Module's parameters or some of them.
 
         This function is idempotent ie unfreezing a model that is not frozen is
@@ -553,7 +589,7 @@ class Module(dict):
             _unfreeze_impl("", self)
         return self
 
-    def train(self, mode: bool = True) -> Module:
+    def train(self, mode: bool = True) -> "Module":
         """Set the model in or out of training mode.
 
         Training mode only applies to certain layers. For example
@@ -573,7 +609,7 @@ class Module(dict):
         self.apply_to_modules(_set_train)
         return self
 
-    def eval(self) -> Module:
+    def eval(self) -> "Module":
         """Set the model to evaluation mode.
 
         See :func:`train`.
@@ -602,39 +638,3 @@ class Module(dict):
                 return True
 
         self.apply(lambda x: x.astype(dtype) if predicate(x.dtype) else x)
-
-
-def _unwrap(model, value_key, value, filter_fn, map_fn, is_leaf_fn):
-    if is_leaf_fn(model, value_key, value):
-        return map_fn(value)
-
-    elif isinstance(value, Module):
-        return {
-            k: _unwrap(value, k, v, filter_fn, map_fn, is_leaf_fn)
-            for k, v in value.items()
-            if filter_fn(value, k, v)
-        }
-
-    elif isinstance(value, dict):
-        nd = {}
-        for k, v in value.items():
-            tk = f"{value_key}.{k}"
-            nd[k] = (
-                _unwrap(model, tk, v, filter_fn, map_fn, is_leaf_fn)
-                if filter_fn(model, tk, v)
-                else {}
-            )
-        return nd
-
-    elif isinstance(value, list):
-        nl = []
-        for i, vi in enumerate(value):
-            tk = f"{value_key}.{i}"
-            nl.append(
-                _unwrap(model, tk, vi, filter_fn, map_fn, is_leaf_fn)
-                if filter_fn(model, tk, vi)
-                else {}
-            )
-        return nl
-
-    raise RuntimeError("Unexpected leaf found while traversing the module")


================================================================================
File: /Users/jluker/mi-experiments/README.md
================================================================================
# MI Experiments

A research project exploring emphasis and ablation techniques in transformer models using MLX. This project combines ideas from parallel inference optimization and representation engineering techniques.

## Features

- Transformer model implementation with MLX
- Configurable emphasis/ablation for:
  - Layers
  - Attention heads
  - Neurons
- Top-p sampling with temperature control
- Structured logging
- Parallel inference optimizations

## Installation

```bash
git clone https://github.com/yourusername/resume-experiments.git
cd resume-experiments
pip install -r requirements.txt
```

## Quick Start

```python
from src.inference.generator import self_sufficient_inference
from src.models import load_model

# Load model
model, tokenizer = load_model("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# Configure emphasis
emphasis_config = {
    'layers': {'0': 1.5, '1': 0.0},
    'heads': {'3': {'1': 2.0, '2': 0.0}},
    'neurons': {'4': {'15': 1.5, '30': 0.0}}
}

# Generate text
response = self_sufficient_inference(
    model=model,
    tokenizer=tokenizer,
    prompts=["Who are you?"],
    emphasis_config=emphasis_config,
    max_tokens=100,
    temp=0.7,
    top_p=0.9
)
```

## Configuration

### Emphasis Configuration

The emphasis configuration allows you to modify the behavior of specific model components:

```json
{
    "layers": {
        "0": 1.5,  // Amplify first layer
        "1": 0.0   // Ablate second layer
    },
    "heads": {
        "3": {     // Configure heads in layer 3
            "1": 2.0,  // Double attention head 1
            "2": 0.0   // Ablate attention head 2
        }
    },
    "neurons": {
        "4": {     // Configure neurons in layer 4
            "15": 1.5, // Amplify neuron 15
            "30": 0.0  // Ablate neuron 30
        }
    }
}
```

## Acknowledgments

This project builds upon several excellent open source works:

- [MLX ParaLLM](https://github.com/willccbb/mlx_parallm) by willccbb - Fast parallel LLM inference techniques
- [MLX Examples](https://github.com/ml-explore/mlx-examples) by Apple - Core MLX implementation and examples
- [repeng](https://github.com/vgel/repeng) by Theia Vogel - Representation engineering implementation
- Original representation engineering research by [Andy Zou et al.](https://github.com/andyzoujm/representation-engineering)

## Citation

If you use this work in academic research, please cite the following:

```bibtex
@misc{zou2024representation,
  title={Representation Engineering: A Top-Down Approach to AI Alignment}, 
  author={Andy Zou and Long Phan and Sarah Chen and James Campbell and Alejandro Escontrela and Ivan Evtimov},
  year={2024},
  eprint={2310.01405},
  archivePrefix={arXiv}
}

@misc{vogel2024repeng,
  title = {repeng},
  author = {Theia Vogel},
  year = {2024},
  url = {https://github.com/vgel/repeng/}
}

@software{mlx2023,
  author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
  title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
  url = {https://github.com/ml-explore},
  version = {0.0},
  year = {2023},
}
```

## License

MIT License - see LICENSE file for details

## Features to make sure it contains

I'll analyze each file and create a comprehensive inventory of functionality. Let me break this down by core components and features.

1. MODEL COMPONENTS

  - Base Classes:
  - BatchedKVCache
  - ModelArgs / BaseModelArgs
  - DynamicNTKScalingRoPE
  - Attention
  - MLP
  - TransformerBlock
  - LlamaModel
  - Model

2. ATTENTION FUNCTIONALITY

- Batched attention implementation
- Unbatched attention implementation
- Grouped Query Attention (GQA)
- Head scaling/emphasis
- Attention metrics (scores, weights, queries, keys, values)
- Head-wise emphasis
- Attention pattern analysis
- Key/Value cache management

3. POSITION ENCODING

- RoPE implementation
- Dynamic NTK scaling
- Traditional vs scaled variants
- Base frequency computation
- Position offset handling

4. MEMORY MANAGEMENT

- KV Cache features:
  - Dynamic resizing
  - Batch-aware caching
  - Offset tracking
  - Memory-efficient concatenation
  - Cache state management
  - Cache updates and fetching

5. MODEL CONFIGURATION

- Config loading/parsing
- ModelArgs initialization
- Parameter validation
- Config validation
- Config import/export
- Configuration persistence
- Model path resolution

6. EMPHASIS/ABLATION

- Layer-level scaling
- Head-level scaling
- Neuron-level scaling
- Emphasis configuration management
- Ablation controls
- Configuration validation

7. ANALYSIS TOOLS

- Token representation analysis
- Embedding extraction
- Layer-wise token tracking
- Attention pattern analysis
- Token relationship analysis
- Activation visualization
- Layer activation analysis

8. INFERENCE

- Text generation
- Batched generation
- Sampling strategies (top-p)
- Temperature control
- Repetition penalty
- Prompt formatting
- Response generation

9. UI COMPONENTS

- Dash app setup
- Interactive controls
- Configuration interface
- Visualization components
- Prompt management
- Response display
- Config import/export UI

10. UTILITIES

- Model loading
- Tokenizer handling
- Weight management
- Mask generation
- Causal mask creation
- Attention mask generation
- Logging setup
- Error handling

11. VISUALIZATION

- Attention pattern plotting
- Token embedding visualization
- Layer activation visualization
- PCA/t-SNE implementations
- Interactive plots
- Real-time visualization

12. SAMPLING

- Top-p sampling
- Temperature sampling
- Logits processing
- Probability normalization
- Token selection

13. LOGGING

- Custom verbose level
- File handlers
- Console handlers
- Formatted messages
- Debug levels
- Performance logging

14. EXPERIMENT TRACKING

- Neuron analysis
- Layer impact analysis
- Head contribution analysis
- Model behavior tracking
- Ablation studies

Would you like me to:

1. Map where each of these features currently exists across the files?
2. Identify which implementations are duplicated?
3. Compare different implementations of the same functionality?
4. Rate the completeness/quality of each implementation?


================================================================================
File: /Users/jluker/mi-experiments/setup.py
================================================================================
from setuptools import setup, find_packages

setup(
    name="mi-experiments",
    version="0.1",
    packages=find_packages(),
    install_requires=[
        # Add your dependencies from requirements.txt here
    ]
) 

================================================================================
File: /Users/jluker/mi-experiments/.gitignore
================================================================================
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Logs
*.log
logs/
*.logs

# Virtual Environment
venv/
ENV/

# IDE
.idea/
.vscode/
*.swp
*.swo

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Model files
*.safetensors
*.bin
*.pt

================================================================================
File: /Users/jluker/mi-experiments/tests/test_inference.py
================================================================================


================================================================================
File: /Users/jluker/mi-experiments/tests/__init__.py
================================================================================


================================================================================
File: /Users/jluker/mi-experiments/tests/test_emphasis.py
================================================================================
import pytest
import mlx.core as mx
from src.models import load_model
from src.inference.generator import generate_with_emphasis
from src.utils.config import EmphasisConfig, ConfigManager

@pytest.fixture
def test_setup():
    model, tokenizer = load_model("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
    config_manager = ConfigManager()
    return model, tokenizer, config_manager

def test_end_to_end_inference(test_setup):
    model, tokenizer, _ = test_setup
    
    # Test basic inference
    prompts = ["Explain what a neural network is."]
    response = generate_with_emphasis(
        model=model,
        tokenizer=tokenizer,
        prompts=prompts,
        config={"max_tokens": 50, "temperature": 0.7, "top_p": 0.9}
    )
    assert isinstance(response[0], str)
    assert len(response[0]) > 0

def test_emphasis_configurations(test_setup):
    model, tokenizer, config_manager = test_setup
    
    # Test configurations
    emphasis_configs = [
        {},  # baseline - no emphasis
        {
            "layers": {
                "0": 1.5,
                "1": 0.0
            }
        },
        {
            "heads": {
                "3": {
                    "1": 2.0,
                    "2": 0.0
                }
            }
        },
        {
            "neurons": {
                "4": {
                    "15": 1.5,
                    "30": 0.0
                }
            }
        },
        {
            "layers": {"0": 1.5},
            "heads": {"3": {"1": 2.0}},
            "neurons": {"4": {"15": 1.5}}
        }
    ]
    
    # Test each configuration
    for config in emphasis_configs:
        response = generate_with_emphasis(
            model=model,
            tokenizer=tokenizer,
            prompts=["Test prompt"],
            config={"max_tokens": 20},
            emphasis_config=config
        )
        assert isinstance(response[0], str)

def test_config_persistence(test_setup):
    _, _, config_manager = test_setup
    
    test_config = EmphasisConfig(
        layers={'0': 1.5, '1': 0.0},
        heads={'3': {'1': 2.0, '2': 0.0}},
        neurons={'4': {'15': 1.5, '30': 0.0}}
    )
    
    config_manager.emphasis_config = test_config
    config_manager.save_config()
    
    new_config_manager = ConfigManager()
    new_config_manager.load_config()
    
    assert new_config_manager.emphasis_config.to_dict() == test_config.to_dict() 

================================================================================
File: /Users/jluker/mi-experiments/tests/test_models.py
================================================================================


================================================================================
File: /Users/jluker/mi-experiments/tests/test_integration.py
================================================================================
import pytest
import mlx.core as mx
from src.models import load_model
from src.inference.generator import generate_with_emphasis
from src.utils.config import EmphasisConfig, ConfigManager

@pytest.fixture
def test_setup():
    model, tokenizer = load_model("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
    config_manager = ConfigManager()
    return model, tokenizer, config_manager

def test_end_to_end_inference(test_setup):
    model, tokenizer, _ = test_setup
    
    # Test basic inference
    prompts = ["Explain what a neural network is."]
    response = generate_with_emphasis(
        model=model,
        tokenizer=tokenizer,
        prompts=prompts,
        config={"max_tokens": 50, "temperature": 0.7, "top_p": 0.9}
    )
    assert isinstance(response[0], str)
    assert len(response[0]) > 0

def test_emphasis_configurations(test_setup):
    model, tokenizer, config_manager = test_setup
    
    # Reference example configurations
    emphasis_configs = []
    
    # Reference from emphasis_example.py
    for config in emphasis_configs:
        response = generate_with_emphasis(
            model=model,
            tokenizer=tokenizer,
            prompts=["Test prompt"],
            config={"max_tokens": 20},
            emphasis_config=config
        )
        assert isinstance(response[0], str)

def test_config_persistence(test_setup):
    _, _, config_manager = test_setup
    
    test_config = EmphasisConfig(
        layers={'0': 1.5, '1': 0.0},
        heads={'3': {'1': 2.0, '2': 0.0}},
        neurons={'4': {'15': 1.5, '30': 0.0}}
    )
    
    config_manager.emphasis_config = test_config
    config_manager.save_config()
    
    new_config_manager = ConfigManager()
    new_config_manager.load_config()
    
    assert new_config_manager.emphasis_config.to_dict() == test_config.to_dict()

================================================================================
File: /Users/jluker/mi-experiments/examples/emphasis_example.py
================================================================================
import json
import os
import sys
from pathlib import Path
from src.models.load_model import load_model
from src.inference.generator import generate_with_emphasis, GenerationConfig
from src.utils.logging import setup_logger

# Set up logging
logger = setup_logger(__name__, "emphasis_experiment.log")

def load_config(config_path: str = "configs/example_emphasis_config.json"):
    """Load experiment configuration from JSON file"""
    with open(config_path) as f:
        return json.load(f)

def run_emphasis_experiment(config_path: str = "configs/example_emphasis_config.json"):
    # Load configuration
    logger.info(f"Loading configuration from {config_path}")
    config = load_config(config_path)
    
    # Load model
    logger.info("Loading model...")
    model, tokenizer = load_model(
        "mlx-community/Mistral-7B-Instruct-v0.3-4bit"
    )

    # Get generation parameters
    gen_params = config["default_generation_params"]
    
    # Create generation config
    generation_config = GenerationConfig(
        max_tokens=gen_params["max_tokens"],
        temperature=gen_params["temperature"],
        top_p=gen_params["top_p"]
    )
    
    # Run experiments for each prompt
    for prompt in config["test_prompts"]:
        logger.info(f"\nTesting prompt: {prompt}")
        logger.info("=" * 80)
        
        # Run each experiment configuration
        for experiment in config["experiments"]:
            logger.info(f"\nRunning experiment: {experiment['name']}")
            logger.info(f"Description: {experiment['description']}")
            logger.info(f"Config: {experiment['config']}")
            
            response = generate_with_emphasis(
                model=model,
                tokenizer=tokenizer,
                prompts=[prompt],
                config=generation_config,
                emphasis_config=experiment["config"],
                verbose=True
            )
            
            logger.info(f"Response: {response[0]}\n")
            logger.info("-" * 80)

if __name__ == "__main__":
    # Allow custom config path via environment variable
    config_path = Path(
        os.getenv(
            "EMPHASIS_CONFIG_PATH", 
            "configs/example_emphasis_config.json"
        )
    )
    
    if not config_path.exists():
        logger.error(f"Config file not found: {config_path}")
        sys.exit(1)
        
    run_emphasis_experiment(str(config_path))

================================================================================
File: /Users/jluker/mi-experiments/examples/basic_inference.py
================================================================================


================================================================================
File: /Users/jluker/mi-experiments/configs/example_emphasis_config.json
================================================================================
{
    "experiments": [
        {
            "name": "baseline",
            "description": "No emphasis or ablation",
            "config": {}
        },
        {
            "name": "layer_emphasis",
            "description": "Amplify first layer, ablate second layer",
            "config": {
                "layers": {
                    "0": 1.5,
                    "1": 0.0
                }
            }
        },
        {
            "name": "head_emphasis",
            "description": "Modify attention heads in layer 3",
            "config": {
                "heads": {
                    "3": {
                        "1": 2.0,
                        "2": 0.0
                    }
                }
            }
        },
        {
            "name": "neuron_emphasis",
            "description": "Modify neurons in layer 4",
            "config": {
                "neurons": {
                    "4": {
                        "15": 1.5,
                        "30": 0.0
                    }
                }
            }
        },
        {
            "name": "combined_emphasis",
            "description": "Combined layer, head, and neuron modifications",
            "config": {
                "layers": {
                    "0": 1.5
                },
                "heads": {
                    "3": {
                        "1": 2.0
                    }
                },
                "neurons": {
                    "4": {
                        "15": 1.5
                    }
                }
            }
        }
    ],
    "default_generation_params": {
        "max_tokens": 100,
        "temperature": 0.7,
        "top_p": 0.9
    },
    "test_prompts": [
        "Explain the concept of neural networks in simple terms.",
        "What is the role of attention in transformer models?",
        "How does machine learning differ from traditional programming?"
    ]
}

================================================================================
File: /Users/jluker/mi-experiments/configs/default_config.json
================================================================================


================================================================================
File: /Users/jluker/mi-experiments/mi_experiments.egg-info/PKG-INFO
================================================================================
Metadata-Version: 2.1
Name: mi-experiments
Version: 0.1
License-File: LICENSE


================================================================================
File: /Users/jluker/mi-experiments/mi_experiments.egg-info/SOURCES.txt
================================================================================
LICENSE
README.md
setup.py
mi_experiments.egg-info/PKG-INFO
mi_experiments.egg-info/SOURCES.txt
mi_experiments.egg-info/dependency_links.txt
mi_experiments.egg-info/top_level.txt
src/__init__.py
src/inference/__init__.py
src/inference/cache.py
src/inference/emphasis.py
src/inference/generator.py
src/inference/sampling.py
src/models/__init__.py
src/models/attention.py
src/models/load_model.py
src/models/mlp.py
src/models/model.py
src/models/transformer.py
src/models/transformer_block.py
src/utils/__init__.py
src/utils/config.py
src/utils/logging.py
src/utils/paths.py
src/utils/tokenizer.py
tests/__init__.py
tests/test_emphasis.py
tests/test_inference.py
tests/test_integration.py
tests/test_models.py

================================================================================
File: /Users/jluker/mi-experiments/mi_experiments.egg-info/top_level.txt
================================================================================
src
tests


================================================================================
File: /Users/jluker/mi-experiments/mi_experiments.egg-info/dependency_links.txt
================================================================================



================================================================================
File: /Users/jluker/mi-experiments/src/__init__.py
================================================================================


================================================================================
File: /Users/jluker/mi-experiments/src/utils/logging.py
================================================================================
import logging

VERBOSE = 15
logging.addLevelName(VERBOSE, "VERBOSE")

def verbose(self, message, *args, **kwargs):
    if self.isEnabledFor(VERBOSE):
        self._log(VERBOSE, message, args, **kwargs)

logging.Logger.verbose = verbose

def setup_logger(name: str, log_file: str = None, level=VERBOSE):
    """Configure and return a logger instance."""
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    # File handler (optional)
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    return logger

================================================================================
File: /Users/jluker/mi-experiments/src/utils/config.py
================================================================================
from dataclasses import dataclass
from typing import Dict, Any, Optional
import json
from pathlib import Path
from ..utils.logging import setup_logger

logger = setup_logger(__name__)

@dataclass
class EmphasisConfig:
    layers: Optional[Dict[str, float]] = None
    heads: Optional[Dict[str, Dict[str, float]]] = None
    neurons: Optional[Dict[str, Dict[str, float]]] = None

    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'EmphasisConfig':
        return cls(
            layers=config_dict.get('layers'),
            heads=config_dict.get('heads'),
            neurons=config_dict.get('neurons')
        )

    def to_dict(self) -> Dict[str, Any]:
        return {
            'layers': self.layers,
            'heads': self.heads,
            'neurons': self.neurons
        }

class ConfigManager:
    def __init__(self, config_path: Optional[Path] = None):
        self.config_path = config_path or Path('configs/default_config.json')
        self.emphasis_config = EmphasisConfig()
        
    def load_config(self) -> None:
        """Load configuration from file"""
        try:
            with open(self.config_path) as f:
                config_dict = json.load(f)
            self.emphasis_config = EmphasisConfig.from_dict(config_dict)
            logger.info(f"Loaded config from {self.config_path}")
        except FileNotFoundError:
            logger.warning(f"Config file not found at {self.config_path}, using defaults")
            
    def save_config(self) -> None:
        """Save current configuration to file"""
        self.config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.config_path, 'w') as f:
            json.dump(self.emphasis_config.to_dict(), f, indent=2)
        logger.info(f"Saved config to {self.config_path}") 

================================================================================
File: /Users/jluker/mi-experiments/src/utils/paths.py
================================================================================
from pathlib import Path
import os

def get_model_path(model_name: str) -> Path:
    """Resolve model path with fallbacks"""
    # Check environment variable first
    if model_dir := os.getenv("MODEL_DIR"):
        path = Path(model_dir) / model_name
        if path.exists():
            return path
            
    # Check common locations
    locations = [
        Path.home() / ".cache/huggingface/hub",
        Path("/models"),
        Path.cwd() / "models"
    ]
    
    for loc in locations:
        path = loc / model_name
        if path.exists():
            return path
            
    raise FileNotFoundError(f"Could not find model: {model_name}") 

================================================================================
File: /Users/jluker/mi-experiments/src/utils/__init__.py
================================================================================


================================================================================
File: /Users/jluker/mi-experiments/src/utils/tokenizer.py
================================================================================
from typing import List, Dict, Any, Union
from transformers import PreTrainedTokenizer
from ..utils.logging import setup_logger

logger = setup_logger(__name__)

class TokenizerWrapper:
    def __init__(self, tokenizer: PreTrainedTokenizer):
        self._tokenizer = tokenizer
        self.pad_token = tokenizer.pad_token
        self.eos_token = tokenizer.eos_token
        self.eos_token_id = tokenizer.eos_token_id
        
    def __call__(self, texts: Union[str, List[str]], **kwargs) -> Dict[str, Any]:
        """Tokenize text(s)"""
        return self._tokenizer(texts, **kwargs)
    
    def decode(self, token_ids: List[int], **kwargs) -> str:
        """Decode token IDs to text"""
        return self._tokenizer.decode(token_ids, **kwargs)
    
    def apply_chat_template(
        self, 
        messages: List[Dict[str, str]], 
        add_generation_prompt: bool = True,
        tokenize: bool = True
    ) -> Union[str, Dict[str, Any]]:
        """Apply chat template to messages"""
        formatted = self._tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=add_generation_prompt
        )
        
        if tokenize:
            return self._tokenizer(formatted)
        return formatted


================================================================================
File: /Users/jluker/mi-experiments/src/models/transformer_block.py
================================================================================
from dataclasses import dataclass
from typing import Optional, Dict, Any, Tuple
import mlx.core as mx
import mlx.nn as nn
from .attention import Attention
from .mlp import MLP
from ..utils.logging import setup_logger

logger = setup_logger(__name__)

class TransformerBlock(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.num_attention_heads = args.num_attention_heads
        self.hidden_size = args.hidden_size
        
        # Main components
        self.self_attn = Attention(args)
        self.mlp = MLP(args)
        
        # Layer norms
        self.input_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        
        # Store args for later reference
        self.args = args
        
        # Layer emphasis scaling (default to 1.0)
        self.layer_scale = mx.array(1.0, dtype=mx.float32)

    def forward(
        self,
        hidden_states: mx.array,
        attention_mask: Optional[mx.array] = None,
        cache: Optional[Tuple[mx.array, mx.array]] = None,
        return_attention_weights: bool = False
    ) -> Tuple[mx.array, Optional[mx.array]]:
        # Apply input layernorm
        normed_hidden_states = self.input_layernorm(hidden_states)
        
        # Self attention
        if return_attention_weights:
            attention_output, attention_weights = self.self_attn(
                normed_hidden_states,
                attention_mask,
                cache=cache,
                return_attention_weights=True
            )
        else:
            attention_output = self.self_attn(
                normed_hidden_states,
                attention_mask,
                cache=cache
            )
            attention_weights = None
        
        # Apply layer emphasis scaling
        attention_output = attention_output * self.layer_scale
        
        # Residual connection
        hidden_states = hidden_states + attention_output
        
        # MLP
        normed_hidden_states = self.post_attention_layernorm(hidden_states)
        mlp_output = self.mlp(normed_hidden_states)
        
        # Apply layer emphasis scaling
        mlp_output = mlp_output * self.layer_scale
        
        # Final residual connection
        hidden_states = hidden_states + mlp_output
        
        if return_attention_weights:
            return hidden_states, attention_weights
        return hidden_states

    def update_emphasis(self, layer_scale: Optional[float] = None):
        """Update emphasis values for this transformer block"""
        if layer_scale is not None:
            self.layer_scale = mx.array(layer_scale, dtype=mx.float32)
            logger.debug(f"Updated layer scale to {layer_scale}") 

================================================================================
File: /Users/jluker/mi-experiments/src/models/load_model.py
================================================================================
from typing import Tuple, Optional
import mlx.core as mx
from transformers import AutoTokenizer, AutoConfig
from huggingface_hub import snapshot_download
from .transformer import ModelArgs, TransformerModel
from ..utils.logging import setup_logger
from ..utils.tokenizer import TokenizerWrapper
import os

logger = setup_logger(__name__)

def quantize_weights(w: mx.array) -> mx.array:
    """Quantize weights to 4-bit precision"""
    # Get the absolute maximum value
    abs_max = mx.max(mx.abs(w))
    
    # Calculate scaling factor for 4-bit quantization
    scale = abs_max / 7.0
    
    # Quantize to 4-bit integers (-7 to 7)
    w_quant = mx.round(w / scale)
    w_quant = mx.clip(w_quant, -7, 7)
    
    # Scale back to original range
    w_dequant = w_quant * scale
    
    return w_dequant

def load_model(
    model_name: str,
    quantize: bool = True,
    device: Optional[str] = None
) -> Tuple[TransformerModel, TokenizerWrapper]:
    """
    Load a pre-trained model and tokenizer
    """
    logger.info(f"Loading model: {model_name}")
    
    # Download model files from HuggingFace
    cache_dir = snapshot_download(
        repo_id=model_name,
        allow_patterns=["*.safetensors", "*.json", "*.model", "tokenizer.json"],
    )
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(cache_dir)
    tokenizer = TokenizerWrapper(tokenizer)
    
    # Load model config and create args
    config = AutoConfig.from_pretrained(cache_dir)
    model_args = ModelArgs(
        model_type=config.model_type,
        hidden_size=config.hidden_size,
        num_hidden_layers=config.num_hidden_layers,
        intermediate_size=config.intermediate_size,
        num_attention_heads=config.num_attention_heads,
        rms_norm_eps=config.rms_norm_eps,
        vocab_size=config.vocab_size,
        head_dim=getattr(config, "head_dim", None),
        num_key_value_heads=getattr(config, "num_key_value_heads", None),
        attention_bias=getattr(config, "attention_bias", False),
        mlp_bias=getattr(config, "mlp_bias", False)
    )
    
    # Initialize model
    model = TransformerModel(model_args)
    
    # Load weights from downloaded files
    weights_path = os.path.join(cache_dir, "weights.safetensors")
    weights = mx.load(weights_path)
    if quantize:
        weights = {k: quantize_weights(v) for k, v in weights.items()}
    model.update(weights)
    
    logger.info("Model loaded successfully")
    return model, tokenizer 

================================================================================
File: /Users/jluker/mi-experiments/src/models/attention.py
================================================================================
import mlx.core as mx
import mlx.nn as nn
from dataclasses import dataclass
from typing import Optional, Tuple, Union
from ..utils.logging import setup_logger

logger = setup_logger(__name__)

class Attention(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.hidden_size = args.hidden_size
        self.n_heads = args.num_attention_heads
        self.n_kv_heads = args.num_key_value_heads or args.num_attention_heads
        self.head_dim = args.head_dim or args.hidden_size // args.num_attention_heads
        
        self.q_proj = nn.Linear(args.hidden_size, self.n_heads * self.head_dim, bias=args.attention_bias)
        self.k_proj = nn.Linear(args.hidden_size, self.n_kv_heads * self.head_dim, bias=args.attention_bias)
        self.v_proj = nn.Linear(args.hidden_size, self.n_kv_heads * self.head_dim, bias=args.attention_bias)
        self.o_proj = nn.Linear(self.n_heads * self.head_dim, args.hidden_size, bias=args.attention_bias)
        
        self.scale = self.head_dim ** -0.5
        
        # Initialize head scaling factors (for emphasis/ablation)
        self.head_scale = mx.array([1.0] * self.n_heads, dtype=mx.float32)

    def forward(self, x: mx.array, mask: Optional[mx.array] = None, 
               return_attention_weights: bool = False) -> Union[mx.array, Tuple[mx.array, mx.array]]:
        B, L, _ = x.shape

        # Project to Q, K, V
        queries = self.q_proj(x).reshape(B, L, self.n_heads, -1).transpose(0, 2, 1, 3)
        keys = self.k_proj(x).reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)
        values = self.v_proj(x).reshape(B, L, self.n_kv_heads, -1).transpose(0, 2, 1, 3)

        # Apply head scaling (including ablation if scaling is zero)
        head_scale = self.head_scale[None, :, None, None]
        queries = queries * head_scale
        if self.n_heads == self.n_kv_heads:
            keys = keys * head_scale
            values = values * head_scale

        # Compute attention
        output = mx.fast.scaled_dot_product_attention(
            queries, keys, values, 
            scale=self.scale,
            mask=mask
        )

        # Reshape and project output
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        output = self.o_proj(output)

        if return_attention_weights:
            if self.n_heads != self.n_kv_heads:
                keys = mx.repeat(keys, self.n_heads // self.n_kv_heads, axis=1)
            attention_weights = mx.softmax(
                (queries @ keys.transpose(0, 1, 3, 2)) * self.scale,
                axis=-1
            )
            return output, attention_weights
        return output


================================================================================
File: /Users/jluker/mi-experiments/src/models/__init__.py
================================================================================


================================================================================
File: /Users/jluker/mi-experiments/src/models/mlp.py
================================================================================
import mlx.core as mx
import mlx.nn as nn
from ..utils.logging import setup_logger

logger = setup_logger(__name__)

class MLP(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.hidden_size = args.hidden_size
        self.intermediate_size = args.intermediate_size
        
        self.gate_proj = nn.Linear(args.hidden_size, args.intermediate_size, bias=args.mlp_bias)
        self.up_proj = nn.Linear(args.hidden_size, args.intermediate_size, bias=args.mlp_bias)
        self.down_proj = nn.Linear(args.intermediate_size, args.hidden_size, bias=args.mlp_bias)
        
        # Initialize neuron scaling factors (for emphasis/ablation)
        self.neuron_scale = mx.array([1.0] * args.intermediate_size, dtype=mx.float32)

    def forward(self, x: mx.array) -> mx.array:
        gate = self.gate_proj(x)
        up = self.up_proj(x)
        
        # Apply neuron scaling (including ablation if scaling is zero)
        neuron_scale = self.neuron_scale[None, None, :]
        gate = gate * neuron_scale
        up = up * neuron_scale
        
        # SwiGLU activation
        intermediate = mx.sigmoid(gate) * up
        
        return self.down_proj(intermediate)


================================================================================
File: /Users/jluker/mi-experiments/src/models/model.py
================================================================================
from dataclasses import dataclass
from typing import Optional, Dict, Union, List, Any
import mlx.core as mx
import mlx.nn as nn
from ..utils.logging import setup_logger
from .transformer_block import TransformerBlock

logger = setup_logger(__name__)

@dataclass
class ModelConfig:
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    head_dim: Optional[int] = None
    max_position_embeddings: Optional[int] = None
    num_key_value_heads: Optional[int] = None
    attention_bias: bool = False
    mlp_bias: bool = False
    rope_theta: float = 10000
    rope_traditional: bool = False
    tie_word_embeddings: bool = True

    @classmethod
    def from_model(cls, model):
        return cls(
            model_type=model.model.args.model_type,
            hidden_size=model.model.args.hidden_size,
            num_hidden_layers=model.model.args.num_hidden_layers,
            intermediate_size=model.model.args.intermediate_size,
            num_attention_heads=model.model.args.num_attention_heads,
            rms_norm_eps=model.model.args.rms_norm_eps,
            vocab_size=model.model.args.vocab_size,
            head_dim=model.head_dim,
            num_key_value_heads=model.n_kv_heads,
            tie_word_embeddings=model.model.args.tie_word_embeddings
        )

class Model(nn.Module):
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = [TransformerBlock(config) for _ in range(config.num_hidden_layers)]
        self.norm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        
        if not config.tie_word_embeddings:
            self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    def set_emphasis_config(self, emphasis_config: Dict[str, Any]):
        """Apply emphasis configuration to model components"""
        from ..inference.emphasis import (
            apply_layer_emphasis,
            apply_head_emphasis,
            apply_neuron_emphasis
        )
        
        if 'layers' in emphasis_config:
            apply_layer_emphasis(self, emphasis_config['layers'])
        if 'heads' in emphasis_config:
            apply_head_emphasis(self, emphasis_config['heads'])
        if 'neurons' in emphasis_config:
            apply_neuron_emphasis(self, emphasis_config['neurons'])

    def forward(self, inputs: mx.array, cache=None, return_hidden_states: bool = False):
        h = self.embed_tokens(inputs)
        
        if cache is None:
            cache = [None] * len(self.layers)
            
        hidden_states = []
        for layer, c in zip(self.layers, cache):
            h, attn_weights, _, _, new_cache = layer(h, cache=c)
            if return_hidden_states:
                hidden_states.append(h)
                
        h = self.norm(h)
        
        if self.config.tie_word_embeddings:
            out = self.embed_tokens.as_linear(h)
        else:
            out = self.lm_head(h)
            
        if return_hidden_states:
            return out, hidden_states, attn_weights
        return out 

================================================================================
File: /Users/jluker/mi-experiments/src/models/transformer.py
================================================================================
from dataclasses import dataclass
from typing import Optional, Dict, Union, List
import mlx.core as mx
import mlx.nn as nn
from .transformer_block import TransformerBlock
from ..utils.logging import setup_logger

logger = setup_logger(__name__)

@dataclass
class ModelArgs:
    model_type: str
    hidden_size: int
    num_hidden_layers: int
    intermediate_size: int
    num_attention_heads: int
    rms_norm_eps: float
    vocab_size: int
    head_dim: Optional[int] = None
    max_position_embeddings: Optional[int] = None
    num_key_value_heads: Optional[int] = None
    attention_bias: bool = False
    mlp_bias: bool = False
    rope_theta: float = 10000
    rope_traditional: bool = False
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    tie_word_embeddings: bool = True

    def __post_init__(self):
        if self.num_key_value_heads is None:
            self.num_key_value_heads = self.num_attention_heads

class TransformerModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model_type = args.model_type
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [TransformerBlock(args) for _ in range(args.num_hidden_layers)]
        self.norm = nn.RMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)

    def __call__(self, inputs: mx.array, cache=None, return_hidden_states: bool = False):
        h = self.embed_tokens(inputs)
        mask = create_attention_mask(h, cache)
        
        if cache is None:
            cache = [None] * len(self.layers)
            
        hidden_states = []
        for layer, c in zip(self.layers, cache):
            h = layer(h, mask, cache=c)
            if return_hidden_states:
                hidden_states.append(h)
                
        h = self.norm(h)
        
        if self.args.tie_word_embeddings:
            out = self.embed_tokens.as_linear(h)
        else:
            out = self.lm_head(h)
            
        if return_hidden_states:
            return out, hidden_states
        return out

def create_attention_mask(hidden_states: mx.array, cache=None) -> mx.array:
    """Create causal attention mask for transformer model"""
    # Get sequence length
    seq_length = hidden_states.shape[1]
    
    # For cached generation, we only need to mask the new token
    if cache is not None and cache[0] is not None:
        mask = mx.ones((1, 1, seq_length))
        return mask
    
    # Create causal mask for full sequence
    mask = mx.triu(mx.ones((seq_length, seq_length)), k=1)
    mask = mx.where(mask == 1, float('-inf'), 0)
    
    # Add batch dimension
    mask = mask.reshape(1, seq_length, seq_length)
    
    return mask


================================================================================
File: /Users/jluker/mi-experiments/src/inference/cache.py
================================================================================
from dataclasses import dataclass
from typing import Optional, List, Tuple
import mlx.core as mx

@dataclass
class BatchedKVCache:
    """Efficient key-value cache for batch processing"""
    keys: List[mx.array]
    values: List[mx.array]
    
    @classmethod
    def create(cls, num_layers: int, batch_size: int, num_heads: int, head_dim: int):
        return cls(
            keys=[[] for _ in range(num_layers)],
            values=[[] for _ in range(num_layers)]
        )
    
    def update(self, layer_idx: int, key: mx.array, value: mx.array):
        self.keys[layer_idx].append(key)
        self.values[layer_idx].append(value)
    
    def get_layer(self, layer_idx: int) -> Tuple[Optional[mx.array], Optional[mx.array]]:
        if not self.keys[layer_idx]:
            return None, None
        return (
            mx.concatenate(self.keys[layer_idx], axis=1),
            mx.concatenate(self.values[layer_idx], axis=1)
        ) 

================================================================================
File: /Users/jluker/mi-experiments/src/inference/__init__.py
================================================================================
from typing import List, Optional, Dict, Any
import mlx.core as mx
from .generator import GenerationConfig, generate_with_emphasis
from ..utils.logging import setup_logger

logger = setup_logger(__name__)

def generate_with_emphasis(
    model,
    tokenizer,
    prompts: List[str],
    max_tokens: int = 100,
    emphasis_config: Optional[Dict[str, Any]] = None,
    temperature: float = 0.7,
    top_p: float = 0.9,
    top_k: Optional[int] = None,
    verbose: bool = False
) -> List[str]:
    """Generate text with emphasis/ablation controls"""
    
    # Apply emphasis if provided
    if emphasis_config:
        model.set_emphasis_config(emphasis_config)

    # Configure generation
    config = GenerationConfig(
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        top_k=top_k
    )

    # Format and tokenize prompts
    formatted_prompts = [
        tokenizer.apply_chat_template(
            [{"role": "user", "content": p}],
            add_generation_prompt=True
        ) for p in prompts
    ]
    
    input_ids = mx.array(tokenizer(
        formatted_prompts,
        padding=True,
        return_tensors="np"
    )['input_ids'])

    # Generate
    outputs = []
    for tokens in generate_with_emphasis(model, input_ids, config):
        outputs.append(tokens)
        if len(outputs) >= config.max_tokens:
            break

    # Decode responses
    output_ids = mx.concatenate(outputs, axis=-1)
    responses = []
    for ids in output_ids:
        text = tokenizer.decode(ids.tolist())
        text = text.split(tokenizer.eos_token)[0]
        responses.append(text)

    if verbose:
        for prompt, response in zip(prompts, responses):
            logger.info(f"Prompt: {prompt}")
            logger.info(f"Response: {response}")
            logger.info("-" * 80)

    return responses


================================================================================
File: /Users/jluker/mi-experiments/src/inference/generator.py
================================================================================
import mlx.core as mx
from typing import List, Optional
from dataclasses import dataclass
from ..utils.logging import setup_logger

logger = setup_logger(__name__)

@dataclass
class GenerationConfig:
    max_tokens: int = 100
    temperature: float = 1.0
    top_p: float = 1.0
    top_k: int = 0
    stop_tokens: List[int] = None

def generate_tokens(model, input_ids, config: GenerationConfig):
    """Generate tokens using the model with the given configuration"""
    # Initialize model state
    logits = model(input_ids)
    
    # Get the last token's logits for each sequence
    next_logits = logits[:, -1, :]
    
    # Apply temperature scaling
    if config.temperature > 0:
        next_logits = next_logits / config.temperature
        
    # Apply top-k sampling
    if config.top_k > 0:
        next_logits = top_k_logits(next_logits, config.top_k)
        
    # Apply top-p sampling
    if config.top_p < 1.0:
        next_logits = top_p_logits(next_logits, config.top_p)
        
    # Sample from the distribution
    probs = mx.softmax(next_logits, axis=-1)
    next_tokens = mx.random.categorical(probs)
    
    # Yield the generated tokens
    yield next_tokens.reshape(1, -1)

def top_k_logits(logits, k):
    """Keep only the top k tokens with highest probability"""
    v, _ = mx.top_k(logits, k)
    v = v[:, -1, None]
    logits = mx.where(logits < v, float('-inf'), logits)
    return logits

def top_p_logits(logits, p):
    """Keep the top tokens with cumulative probability >= p"""
    sorted_logits, sorted_indices = mx.sort(logits, axis=-1, descending=True)
    cumulative_probs = mx.cumsum(mx.softmax(sorted_logits, axis=-1), axis=-1)
    
    # Remove tokens with cumulative probability above the threshold
    sorted_indices_to_remove = cumulative_probs > p
    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].copy()
    sorted_indices_to_remove[:, 0] = 0
    
    # Scatter sorted tensors to original indexing
    indices_to_remove = mx.zeros_like(logits, dtype=mx.bool_)
    indices_to_remove = mx.scatter(indices_to_remove, sorted_indices, sorted_indices_to_remove, axis=-1)
    logits = mx.where(indices_to_remove, float('-inf'), logits)
    return logits

def generate_with_emphasis(
    model,
    tokenizer,
    prompts: List[str],
    config: GenerationConfig,
    emphasis_config: dict = None,
    verbose: bool = False
) -> List[str]:
    """
    Generate text with emphasis/ablation controls
    """
    if emphasis_config:
        from .emphasis import (
            apply_layer_emphasis,
            apply_head_emphasis,
            apply_neuron_emphasis
        )
        
        if 'layers' in emphasis_config:
            apply_layer_emphasis(model, emphasis_config['layers'])
        if 'heads' in emphasis_config:
            apply_head_emphasis(model, emphasis_config['heads'])
        if 'neurons' in emphasis_config:
            apply_neuron_emphasis(model, emphasis_config['neurons'])

    # Format prompts
    prompts_fm = [[{"role": "user", "content": p}] for p in prompts]
    prompts_fm = [
        tokenizer.apply_chat_template(p, add_generation_prompt=True, tokenize=False)
        for p in prompts_fm
    ]

    # Tokenize
    encoded = tokenizer(prompts_fm, padding=True, return_tensors="np")
    input_ids = mx.array(encoded['input_ids'])

    # Generate
    outputs = []
    for tokens in generate_tokens(model, input_ids, config):
        outputs.append(tokens)
        if len(outputs) >= config.max_tokens:
            break

    output_ids = mx.concatenate(outputs, axis=-1)
    
    # Decode
    responses = []
    for ids in output_ids:
        text = tokenizer.decode(ids.tolist())
        text = text.split(tokenizer.eos_token)[0]
        responses.append(text)

    if verbose:
        for prompt, response in zip(prompts, responses):
            logger.info(f"Prompt: {prompt}")
            logger.info(f"Response: {response}")
            logger.info("-" * 80)

    return responses


================================================================================
File: /Users/jluker/mi-experiments/src/inference/sampling.py
================================================================================
import mlx.core as mx

def top_p_sampling(logits: mx.array, top_p: float = 0.9) -> mx.array:
    """
    Nucleus (top-p) sampling implementation
    """
    probs = mx.softmax(logits, axis=-1)
    sorted_probs = mx.sort(probs, axis=-1, descending=True)
    cumsum_probs = mx.cumsum(sorted_probs, axis=-1)
    mask = cumsum_probs <= top_p
    
    # Keep at least one token
    mask = mx.logical_or(mask, mx.arange(mask.shape[-1]) == 0)
    
    sorted_mask = mx.sort_permutation(probs, axis=-1, descending=True)
    reverse_mask = mx.sort_permutation(sorted_mask, axis=-1)
    
    masked_probs = mx.where(
        mask[reverse_mask],
        probs,
        mx.zeros_like(probs)
    )
    
    # Renormalize
    masked_probs = masked_probs / mx.sum(masked_probs, axis=-1, keepdims=True)
    return masked_probs 

================================================================================
File: /Users/jluker/mi-experiments/src/inference/emphasis.py
================================================================================
from typing import Dict, Any
import mlx.core as mx

def apply_layer_emphasis(model, layer_config: Dict[str, float]):
    """Apply emphasis/ablation to model layers."""
    for layer_idx_str, scaling_factor in layer_config.items():
        layer_idx = int(layer_idx_str)
        if 0 <= layer_idx < len(model.layers):
            scaling_factor_array = mx.array(scaling_factor, dtype=mx.float32)
            model.layers[layer_idx].layer_scale = scaling_factor_array

def apply_head_emphasis(model, head_config: Dict[str, Dict[str, float]]):
    """Apply emphasis/ablation to attention heads."""
    for layer_idx_str, heads_info in head_config.items():
        layer_idx = int(layer_idx_str)
        if 0 <= layer_idx < len(model.layers):
            head_scale = model.layers[layer_idx].self_attn.head_scale
            for head_idx_str, scaling_factor in heads_info.items():
                head_idx = int(head_idx_str)
                if 0 <= head_idx < len(head_scale):
                    scaling_factor_array = mx.array(
                        scaling_factor, 
                        dtype=head_scale.dtype
                    )
                    head_scale[head_idx] = scaling_factor_array
            model.layers[layer_idx].self_attn.head_scale = head_scale

def apply_neuron_emphasis(model, neuron_config: Dict[str, Dict[str, float]]):
    """Apply emphasis/ablation to MLP neurons."""
    for layer_idx_str, neurons_info in neuron_config.items():
        layer_idx = int(layer_idx_str)
        if 0 <= layer_idx < len(model.layers):
            neuron_scale = model.layers[layer_idx].mlp.neuron_scale
            for neuron_idx_str, scaling_factor in neurons_info.items():
                neuron_idx = int(neuron_idx_str)
                if 0 <= neuron_idx < len(neuron_scale):
                    scaling_factor_array = mx.array(
                        scaling_factor, 
                        dtype=neuron_scale.dtype
                    )
                    neuron_scale[neuron_idx] = scaling_factor_array
            model.layers[layer_idx].mlp.neuron_scale = neuron_scale

