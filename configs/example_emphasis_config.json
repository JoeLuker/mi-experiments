{
    "experiments": [
        {
            "name": "baseline",
            "description": "No emphasis or ablation",
            "config": {}
        },
        {
            "name": "layer_emphasis",
            "description": "Amplify first layer, ablate second layer",
            "config": {
                "layers": {
                    "0": 1.5,
                    "1": 0.0
                }
            }
        },
        {
            "name": "head_emphasis",
            "description": "Modify attention heads in layer 3",
            "config": {
                "heads": {
                    "3": {
                        "1": 2.0,
                        "2": 0.0
                    }
                }
            }
        },
        {
            "name": "neuron_emphasis",
            "description": "Modify neurons in layer 4",
            "config": {
                "neurons": {
                    "4": {
                        "15": 1.5,
                        "30": 0.0
                    }
                }
            }
        },
        {
            "name": "combined_emphasis",
            "description": "Combined layer, head, and neuron modifications",
            "config": {
                "layers": {
                    "0": 1.5
                },
                "heads": {
                    "3": {
                        "1": 2.0
                    }
                },
                "neurons": {
                    "4": {
                        "15": 1.5
                    }
                }
            }
        }
    ],
    "default_generation_params": {
        "max_tokens": 100,
        "temperature": 0.7,
        "top_p": 0.9
    },
    "test_prompts": [
        "Explain the concept of neural networks in simple terms.",
        "What is the role of attention in transformer models?",
        "How does machine learning differ from traditional programming?"
    ]
}